{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printMeasurements(confusion_matrix) :\n",
    "    # Calculate metrics for each class\n",
    "    lists = []\n",
    "    for i in range(0,confusion_matrix.shape[0]):\n",
    "        # True Positives (TP): The number of correctly predicted instances for class i\n",
    "        TP = confusion_matrix[i, i]\n",
    "        \n",
    "        \n",
    "        # False Positives (FP): The number of instances predicted as class i but actually belonging to other classes\n",
    "        FP = np.sum(confusion_matrix[:, i]) - TP\n",
    "        \n",
    "        # False Negatives (FN): The number of instances belonging to class i but predicted as other classes\n",
    "        FN = np.sum(confusion_matrix[i, :]) - TP\n",
    "        \n",
    "        # True Negatives (TN): The number of correctly predicted instances not belonging to class i\n",
    "        TN = np.sum(confusion_matrix) - (TP + FP + FN)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        acc = (TP + TN) / (TP + TN + FP + FN)\n",
    "        # lists.append(['Accuracy',acc])\n",
    "        \n",
    "        # Calculate sensitivity (True Positive Rate or Recall)\n",
    "        sens = TP / (TP + FN)\n",
    "        lists.append(['Sesitivity',sens])\n",
    "        \n",
    "        \n",
    "        # Calculate specificity (True Negative Rate)\n",
    "        spec = TN / (TN + FP)\n",
    "        lists.append(['Specificity',spec])\n",
    "        \n",
    "        \n",
    "        # Calculate precision (Positive Predictive Value)\n",
    "        prec = TP / (TP + FP)\n",
    "        lists.append(['Precision',prec])\n",
    "                \n",
    "        \n",
    "        # Calculate recall (True Positive Rate)\n",
    "        rec = sens\n",
    "        # print('Recall For class ',i,' ',rec)\n",
    "        \n",
    "        \n",
    "        # Calculate F1 score\n",
    "        f1 = (2 * prec * rec) / (prec + rec)\n",
    "        lists.append(['F1',f1])\n",
    "    return lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6910842882953186\n"
     ]
    }
   ],
   "source": [
    "cm = [[4678,124,21899,2750],\n",
    "      [62,22428,367,6597],\n",
    "      [502,323,25302,3294],\n",
    "      [39,355,107,29066]]\n",
    "\n",
    "cm = np.array(cm)\n",
    "\n",
    "total = np.sum(cm)\n",
    "diagonal = np.trace(cm)\n",
    "print(diagonal / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
